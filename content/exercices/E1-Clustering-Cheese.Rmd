---
title: "Classification on Cheese dataset: Hierarchical Agglomerative Clustering (HAC) and K-means"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    theme: yeti
    toc: true
    toc_depth: 4
    toc_float: true
---

**Tags:** #Exercice #Clustering #HAC #Kmeans

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r echo=FALSE}
library(GGally)
library(kableExtra)
library(dplyr)
```

***
### Introduction
_Fromage_ dataset is a dataset containing 29 fromages and their nutritional information.
The goal of this exercice is to determine groups of homogenous cheeses based on the nutritional information.

I'm using Hierarchical Agglomerative Clustering (HAC) and K-means algorithms on this dataset.

_This exercice is taken from the [teaching page of Marie Chavent](http://www.math.u-bordeaux.fr/~mchave100p/teaching/) (University of Bordeaux)._
\

### Importing dataset. Descriptive Statistics

```{r}
dta <- read.table("DATA/fromage.txt", header=TRUE, row.names=1)
```

```{r}
head(dta) %>%
  kbl(caption = "Extract of Cheese dataset") %>%
  kable_minimal(font_size = 12, bootstrap_options=c("stripped", "hover"), full_width=F, position="left")
```

```{r echo=FALSE}
summary(dta) %>%
  kbl(caption = "Basic statistics") %>%
  kable_minimal(font_size = 12, bootstrap_options=c("stripped", "hover"), full_width=F, position="left")
```

```{r echo=FALSE}
std_dev <- round(apply(dta, 2, sd), 2)
std_dev <- std_dev[order(std_dev)]
std_dev %>%
  kbl(caption = "Quick glance at standard deviation for each variable", col.names = c("Standard deviation"), align="l") %>%
  kable_minimal(font_size = 12, bootstrap_options=c("stripped", "hover"), full_width=F, position="left")
```
\

- Range of values values differ strongly between variables
- Standard deviation ranges from approximatively 7 to 108 units
- No missing values for any of the variables
\
\

Given the variations of the variables standardization is necessary.
\
\

#### Correlations
```{r fig.height=7, fig.width=10, message=FALSE}
ggpairs(dta)
```
\

There seems to be strong links between some variables. For example we have a strong positive correlation between _calories_ and _lipides_, or _lipides_ and _cholesterol_, or a negative correlation between _calcium_ and _folates_.
\
\

#### Data standardization
```{r}
n <- nrow(dta)
dta_standard <- as.data.frame(scale(dta, center=TRUE,scale=TRUE)*sqrt(n/(n-1)))

round(dta_standard[1:4,1:4], 2) %>%
  kbl(caption = "Quick glance at standardized data", align="l") %>%
  kable_minimal(font_size = 12, bootstrap_options=c("stripped", "hover"), full_width=F, position="left")

summary(dta_standard) %>%
  kbl(caption = "Basic statistics on standardized data", align="l") %>%
  kable_minimal(font_size = 12, bootstrap_options=c("stripped", "hover"), full_width=F, position="left")
```
\

### Hierarchical Agglomerative Clustering (HAC) using Ward algorithm
```{r}
# calculating euclidean distance
d <- dist(dta_standard) # calculating the distance

# HAC
tree <- hclust(d^2/(2*n), method="ward.D")
```
\

#### Choosing number of classes
```{r fig.width=10, fig.height=6}
par(mfrow=c(1,2))
barplot(sort(tree$height, decreasing = TRUE), main="Heights of the tree")
abline(h=0.28, col="red")
abline(h=0.53, col="lightblue")
abline(h=1.07, col="orange")
abline(h=1.59, col="green")

plot(tree, hang=-1, main="HAC Ward", sub="", xlab="")
rect.hclust(tree, k=4, border="blue")
abline(h=0.28, col="red")
abline(h=0.53, col="lightblue")
abline(h=1.07, col="orange")
abline(h=1.59, col="green")
```
\

Judgind by the heights of the tree branches (barplot), last important loss of information happens between 3 and 4 classes, so 4 is retained.
\
\

#### Assigning cheeses to respective classes based on the HAC tree
A new variable _hac_ is created to store cheeses and assignes classes. Then the classes and the original dataset (dta) are merged in _hac_ object.
```{r}
K <- 4 # number of classes
hac <- cutree(tree,k=K)

hac <- as.data.frame(as.factor(hac))
hac <- cbind(hac, dta)
colnames(hac)[1] <- c("cluster_HAC")
```
\

This classification explains `r round(sum(sort(tree$height, decreasing=TRUE)[1:4])/sum(tree$height)*100, 2)`% of total inertia, which is a good representaition of information.
\
\

### Clustering with K-means
I'm using the results of HAC to define the initial number of clusters. K-means is initialized with 4 classes.
```{r}
km <- kmeans(dta, centers=4) # k-means with 4
```
\

The classes and the original dataset (_dta_) are merged in _k.means_ object.
```{r}
k.means <- as.data.frame(as.factor(km$cluster), row.names = FALSE) # to get the results of clustering
k.means <- cbind(k.means, dta)
colnames(k.means)[1] <- c("cluster_KM")
```
\

This classification explains `r round(km$betweenss/km$totss*100, 2)`% of total inertia, which is slightly lower than HAC, but remains a good representation.
\
\

### Quick comparaison of HAC and K-means clustering
```{r}
# Data preparation
temp <- cutree(tree, 4)
dta_clustering <- cbind(as.factor(temp), as.factor(km$cluster), dta)
colnames(dta_clustering)[1:2] <- c("cluster_HAC", "cluster_KM")

hac.table <- table(dta_clustering$cluster_KM, dnn="cluster")
km.table <- table(dta_clustering$cluster_HAC, dnn="cluster")
```

```{r}
kbl(list(hac.table, km.table), caption = "HAC vs. K-means classification results", align="l") %>%
  kable_minimal(font_size = 12, bootstrap_options=c("stripped", "hover"), full_width=F, position="left")
```
\

With 4 classes we get one HAC cluster that has higher effectifs - 17 out of 29 cheeses are part of it. All the other ones have the same number of individials.
With K-means we get three clusters that are fairly equal in size and one that is smaller (4 observations - contains soft cheeses and yoghurts).
\

```{r}
table(dta_clustering$cluster_HAC,dta_clustering$cluster_KM)
```
\

We have only one cluster that contains exactly the same observations: cluster 3 (K-means) and cluster 4 (HAC): `r rownames(dta_clustering)[dta_clustering$cluster_KM==3 & dta_clustering$cluster_HAC==4]`
\

Let's look at the details now: which cheeses are in which cluster.
```{r}
sapply(unique(dta_clustering$cluster_HAC), function(g) rownames(dta_clustering)[dta_clustering$cluster_HAC==g]) %>%
  kbl(caption = "Clustering results - HAC algorithm", align="l") %>%
  kable_minimal(font_size = 12, bootstrap_options=c("stripped", "hover"), full_width=F, position="left")
```
\

```{r}
sapply(unique(dta_clustering$cluster_KM), function(g) rownames(dta_clustering)[dta_clustering$cluster_KM==g]) %>%
  kbl(caption = "Clustering results - K-means algorithm", align="l") %>%
  kable_minimal(font_size = 12, bootstrap_options=c("stripped", "hover"), full_width=F, position="left")
```
\

Overall, the two classifications are quite different. The only class that is exactly the same is the class of cream cheeses and yogurts.
\
\
\